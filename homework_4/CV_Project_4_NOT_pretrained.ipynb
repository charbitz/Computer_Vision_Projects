{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1efK3dBnSNsW",
        "outputId": "225a3d51-c9f4-41ee-8d8e-61fcd5dbc060"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Mount google drive :\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "UES-_QQCSXQM"
      },
      "outputs": [],
      "source": [
        "# Unzip the dataset and save it to the path \"/content/imagedb_btsd\" :\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "local_zip = '/content/drive/MyDrive/cv_proj_4_dataset/imagedb_btsd.zip'\n",
        "\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/content/imagedb_btsd')\n",
        "zip_ref.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "23UpZ_4lTZGN"
      },
      "outputs": [],
      "source": [
        "# Select the folder \"imagedb\" as the training folder\n",
        "# and the folder \"imagedb_test\" as the testing folder :\n",
        "\n",
        "base_dir = '/content/imagedb_btsd'\n",
        "\n",
        "train_dir = os.path.join(base_dir, 'imagedb')\n",
        "test_dir = os.path.join(base_dir, 'imagedb_test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VAg9eIeSShP9",
        "outputId": "21fc8165-6d27-4abd-9eae-41c584045440"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 254, 254, 32)      896       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 127, 127, 32)     0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 125, 125, 64)      18496     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 62, 62, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 60, 60, 64)        36928     \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 30, 30, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 28, 28, 128)       73856     \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 14, 14, 128)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 25088)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1024)              25691136  \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 34)                34850     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 25,856,162\n",
            "Trainable params: 25,856,162\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "from keras import optimizers\n",
        "\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (3, 3), padding=\"valid\", activation='relu', input_shape=(256, 256, 3)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(layers.Conv2D(64, (3, 3), padding=\"valid\", activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(layers.Conv2D(64, (3, 3), padding=\"valid\", activation='relu'))\n",
        "# model.add(layers.Conv2D(128, (3, 3), padding=\"valid\", activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(layers.Conv2D(128, (3, 3), padding=\"valid\", activation='relu'))\n",
        "# model.add(layers.Conv2D(256, (3, 3), padding=\"valid\", activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(1024, activation='relu'))\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(34, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer=optimizers.adam_v2.Adam(learning_rate=1e-4),\n",
        "              loss = 'categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWGtpHkotMbZ",
        "outputId": "697e7342-2933-467d-f02d-88d6b68063c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<keras.layers.convolutional.Conv2D object at 0x7f14a041a850> True\n",
            "<keras.layers.pooling.MaxPooling2D object at 0x7f142b59bb50> True\n",
            "<keras.layers.convolutional.Conv2D object at 0x7f142b5af150> True\n",
            "<keras.layers.pooling.MaxPooling2D object at 0x7f14201acc50> True\n",
            "<keras.layers.convolutional.Conv2D object at 0x7f1420161e50> True\n",
            "<keras.layers.pooling.MaxPooling2D object at 0x7f1420165d90> True\n",
            "<keras.layers.convolutional.Conv2D object at 0x7f142b5af4d0> True\n",
            "<keras.layers.pooling.MaxPooling2D object at 0x7f142016bb10> True\n",
            "<keras.layers.core.flatten.Flatten object at 0x7f14200f7dd0> True\n",
            "<keras.layers.core.dense.Dense object at 0x7f14200f7e10> True\n",
            "<keras.layers.core.dropout.Dropout object at 0x7f1420165410> True\n",
            "<keras.layers.core.dense.Dense object at 0x7f142018fdd0> True\n"
          ]
        }
      ],
      "source": [
        "# check this :\n",
        "\n",
        "# Check the trainable status of the individual layers\n",
        "for layer in model.layers:\n",
        "    print(layer, layer.trainable)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWDZ2ZpnShK9",
        "outputId": "1e5aab11-7686-4091-d8c0-920063826cda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2457 images belonging to 34 classes.\n",
            "Found 599 images belonging to 34 classes.\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(rotation_range = 90,\n",
        "                                  horizontal_flip=True,\n",
        "                                  vertical_flip = True,\n",
        "                                   brightness_range=[0.2,1.0],\n",
        "                                   zoom_range=[0.5,1.0],                                   \n",
        "                                   validation_split=0.2)\n",
        "                                  #  preprocessing_function = preprocess_func)\n",
        "                                  \n",
        "#train_datagen  = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
        "\n",
        "# --------------------\n",
        "# Flow training images in batches using train_datagen generator\n",
        "# --------------------\n",
        "train_generator = train_datagen.flow_from_directory(train_dir,\n",
        "                                                    batch_size=64,\n",
        "                                                    class_mode='categorical',\n",
        "                                                    # color_mode='grayscale',\n",
        "                                                    target_size=(256,256),\n",
        "                                                    shuffle=True,\n",
        "                                                    subset='training', seed=1)     \n",
        "# --------------------\n",
        "# Flow validation images in batches using test_datagen generator\n",
        "# --------------------\n",
        "validation_generator =  train_datagen.flow_from_directory(train_dir,\n",
        "                                                          batch_size=64,\n",
        "                                                          class_mode='categorical',\n",
        "                                                          # color_mode='grayscale',\n",
        "                                                          target_size=(256,256),\n",
        "                                                          subset='validation', seed=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ueQ8b0eukBoi"
      },
      "outputs": [],
      "source": [
        "# Apply callbacks :\n",
        "\n",
        "import datetime \n",
        "import tensorflow as tf\n",
        "  \n",
        "my_callbacks = []\n",
        "\n",
        "# logdir = os.path.join(\"/content/logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "# tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
        "# my_callbacks.append(tensorboard_callback)\n",
        "\n",
        "save_best_callback = tf.keras.callbacks.ModelCheckpoint(f'model_from_scratch_best.hdf5', save_best_only=True, verbose=1)\n",
        "my_callbacks.append(save_best_callback)\n",
        "\n",
        "early_stop_callback = tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True, verbose=1)\n",
        "my_callbacks.append(early_stop_callback)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5iuGSf2TzI_",
        "outputId": "a8952ee5-f754-4196-ad03-e34410b6c2f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "39/38 [==============================] - ETA: 0s - loss: 10.9745 - accuracy: 0.1591WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 15 batches). You may need to use the repeat() function when building your dataset.\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 2.45023, saving model to model_from_scratch_best.hdf5\n",
            "38/38 [==============================] - 59s 1s/step - loss: 10.9745 - accuracy: 0.1591 - val_loss: 2.4502 - val_accuracy: 0.3907\n",
            "Epoch 2/30\n",
            "39/38 [==============================] - ETA: 0s - loss: 2.3671 - accuracy: 0.3915WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "38/38 [==============================] - 39s 1s/step - loss: 2.3671 - accuracy: 0.3915\n",
            "Epoch 3/30\n",
            "39/38 [==============================] - ETA: 0s - loss: 1.9330 - accuracy: 0.4925WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "38/38 [==============================] - 36s 936ms/step - loss: 1.9330 - accuracy: 0.4925\n",
            "Epoch 4/30\n",
            "39/38 [==============================] - ETA: 0s - loss: 1.6246 - accuracy: 0.5674WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "38/38 [==============================] - 36s 936ms/step - loss: 1.6246 - accuracy: 0.5674\n",
            "Epoch 5/30\n",
            "39/38 [==============================] - ETA: 0s - loss: 1.3730 - accuracy: 0.6357WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "38/38 [==============================] - 36s 937ms/step - loss: 1.3730 - accuracy: 0.6357\n",
            "Epoch 6/30\n",
            "39/38 [==============================] - ETA: 0s - loss: 1.2589 - accuracy: 0.6756WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "38/38 [==============================] - 36s 934ms/step - loss: 1.2589 - accuracy: 0.6756\n",
            "Epoch 7/30\n",
            "39/38 [==============================] - ETA: 0s - loss: 1.0980 - accuracy: 0.6996WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "38/38 [==============================] - 36s 937ms/step - loss: 1.0980 - accuracy: 0.6996\n",
            "Epoch 8/30\n",
            "39/38 [==============================] - ETA: 0s - loss: 1.0647 - accuracy: 0.7192WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "38/38 [==============================] - 36s 929ms/step - loss: 1.0647 - accuracy: 0.7192\n",
            "Epoch 9/30\n",
            "39/38 [==============================] - ETA: 0s - loss: 0.9150 - accuracy: 0.7436WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "38/38 [==============================] - 36s 949ms/step - loss: 0.9150 - accuracy: 0.7436\n",
            "Epoch 10/30\n",
            "39/38 [==============================] - ETA: 0s - loss: 0.8852 - accuracy: 0.7656WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "38/38 [==============================] - 36s 928ms/step - loss: 0.8852 - accuracy: 0.7656\n",
            "Epoch 11/30\n",
            "39/38 [==============================] - ETA: 0s - loss: 0.7881 - accuracy: 0.7814WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "38/38 [==============================] - 36s 930ms/step - loss: 0.7881 - accuracy: 0.7814\n",
            "Epoch 12/30\n",
            "39/38 [==============================] - ETA: 0s - loss: 0.8199 - accuracy: 0.7814WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "38/38 [==============================] - 36s 929ms/step - loss: 0.8199 - accuracy: 0.7814\n",
            "Epoch 13/30\n",
            "39/38 [==============================] - ETA: 0s - loss: 0.7234 - accuracy: 0.8059WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "38/38 [==============================] - 36s 928ms/step - loss: 0.7234 - accuracy: 0.8059\n",
            "Epoch 14/30\n",
            "39/38 [==============================] - ETA: 0s - loss: 0.6996 - accuracy: 0.8010WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "38/38 [==============================] - 36s 927ms/step - loss: 0.6996 - accuracy: 0.8010\n",
            "Epoch 15/30\n",
            "39/38 [==============================] - ETA: 0s - loss: 0.6609 - accuracy: 0.8185WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "38/38 [==============================] - 36s 929ms/step - loss: 0.6609 - accuracy: 0.8185\n",
            "Epoch 16/30\n",
            "39/38 [==============================] - ETA: 0s - loss: 0.6054 - accuracy: 0.8356WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "38/38 [==============================] - 36s 933ms/step - loss: 0.6054 - accuracy: 0.8356\n",
            "Epoch 17/30\n",
            "39/38 [==============================] - ETA: 0s - loss: 0.6301 - accuracy: 0.8376WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "38/38 [==============================] - 36s 929ms/step - loss: 0.6301 - accuracy: 0.8376\n",
            "Epoch 18/30\n",
            "39/38 [==============================] - ETA: 0s - loss: 0.5738 - accuracy: 0.8462WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "38/38 [==============================] - 44s 1s/step - loss: 0.5738 - accuracy: 0.8462\n",
            "Epoch 19/30\n",
            "39/38 [==============================] - ETA: 0s - loss: 0.5311 - accuracy: 0.8592WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "38/38 [==============================] - 36s 938ms/step - loss: 0.5311 - accuracy: 0.8592\n",
            "Epoch 20/30\n",
            "39/38 [==============================] - ETA: 0s - loss: 0.5490 - accuracy: 0.8584WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "38/38 [==============================] - 37s 974ms/step - loss: 0.5490 - accuracy: 0.8584\n",
            "Epoch 21/30\n",
            "39/38 [==============================] - ETA: 0s - loss: 0.5141 - accuracy: 0.8645WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "38/38 [==============================] - 36s 937ms/step - loss: 0.5141 - accuracy: 0.8645\n",
            "Epoch 22/30\n",
            "39/38 [==============================] - ETA: 0s - loss: 0.4899 - accuracy: 0.8649WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "38/38 [==============================] - 36s 944ms/step - loss: 0.4899 - accuracy: 0.8649\n",
            "Epoch 23/30\n",
            "39/38 [==============================] - ETA: 0s - loss: 0.4569 - accuracy: 0.8742WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "38/38 [==============================] - 36s 939ms/step - loss: 0.4569 - accuracy: 0.8742\n",
            "Epoch 24/30\n",
            "39/38 [==============================] - ETA: 0s - loss: 0.4472 - accuracy: 0.8685WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "38/38 [==============================] - 36s 939ms/step - loss: 0.4472 - accuracy: 0.8685\n",
            "Epoch 25/30\n",
            "39/38 [==============================] - ETA: 0s - loss: 0.4310 - accuracy: 0.8787WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "38/38 [==============================] - 36s 940ms/step - loss: 0.4310 - accuracy: 0.8787\n",
            "Epoch 26/30\n",
            "39/38 [==============================] - ETA: 0s - loss: 0.4284 - accuracy: 0.8779WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "38/38 [==============================] - 36s 938ms/step - loss: 0.4284 - accuracy: 0.8779\n",
            "Epoch 27/30\n",
            "39/38 [==============================] - ETA: 0s - loss: 0.3942 - accuracy: 0.8836WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "38/38 [==============================] - 36s 938ms/step - loss: 0.3942 - accuracy: 0.8836\n",
            "Epoch 28/30\n",
            "39/38 [==============================] - ETA: 0s - loss: 0.3904 - accuracy: 0.8909WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "38/38 [==============================] - 36s 943ms/step - loss: 0.3904 - accuracy: 0.8909\n",
            "Epoch 29/30\n",
            "39/38 [==============================] - ETA: 0s - loss: 0.4157 - accuracy: 0.8877WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "38/38 [==============================] - 36s 941ms/step - loss: 0.4157 - accuracy: 0.8877\n",
            "Epoch 30/30\n",
            "39/38 [==============================] - ETA: 0s - loss: 0.3539 - accuracy: 0.8995WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "38/38 [==============================] - 36s 939ms/step - loss: 0.3539 - accuracy: 0.8995\n"
          ]
        }
      ],
      "source": [
        "history = model.fit_generator(train_generator,\n",
        "                              validation_data=validation_generator,\n",
        "                              # steps_per_epoch=50,\n",
        "                              steps_per_epoch=train_generator.samples/train_generator.batch_size,\n",
        "                              epochs=30,\n",
        "                              validation_steps=15,\n",
        "                              verbose=1,\n",
        "                              callbacks=my_callbacks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "BZANTHekSznu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d20356a9-b44b-45cd-eb6d-5444fd4476ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2149 images belonging to 34 classes.\n"
          ]
        }
      ],
      "source": [
        "# test_datagen  = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen  = ImageDataGenerator()\n",
        "# --------------------\n",
        "# Flow testing images in batches using test_datagen generator\n",
        "# --------------------\n",
        "test_generator =  test_datagen.flow_from_directory(test_dir,\n",
        "                                                   batch_size=10,\n",
        "                                                   class_mode='categorical',\n",
        "                                                  #  color_mode='grayscale',\n",
        "                                                   target_size=(256,256))\n",
        "# # Testing the CNN on testing data : \n",
        "# loss, acc = model.evaluate(test_generator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Q3PTISh9XtGh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "outputId": "7a28746a-2a7b-4be7-9a09-e757eab97b72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "215/215 [==============================] - 3s 15ms/step - loss: 1.6427 - accuracy: 0.7073\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['accuracy'], label='accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([0, 1])\n",
        "plt.legend(loc='lower right')\n",
        "\n",
        "# Testing the CNN on testing data : \n",
        "loss, acc = model.evaluate(test_generator)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Loading the best state model from the content folder : \n",
        "\n",
        "model_saved = models.load_model('/content/model_from_scratch_best.hdf5')\n",
        "model_saved.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmXJP6VsLlS9",
        "outputId": "d0442add-5e08-45c3-fb0c-cc4a06845ae4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 254, 254, 32)      896       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 127, 127, 32)     0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 125, 125, 64)      18496     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 62, 62, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 60, 60, 64)        36928     \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 30, 30, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 28, 28, 128)       73856     \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 14, 14, 128)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 25088)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1024)              25691136  \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 34)                34850     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 25,856,162\n",
            "Trainable params: 25,856,162\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " loss_saved, acc_saved = model_saved.evaluate(test_generator)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gD96ihm6LwpB",
        "outputId": "a68706df-0db9-47b7-efc3-a442240cfb58"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "215/215 [==============================] - 3s 14ms/step - loss: 2.7700 - accuracy: 0.2834\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "CV_Project_4_NOT_pretrained.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNhfydtsKXiE+AKUk6D+dhU"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}