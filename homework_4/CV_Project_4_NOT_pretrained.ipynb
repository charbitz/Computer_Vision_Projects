{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1efK3dBnSNsW",
        "outputId": "b23dc2d0-fd0a-423b-85bc-2b02ee224bb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Mount google drive :\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "UES-_QQCSXQM"
      },
      "outputs": [],
      "source": [
        "# Unzip the dataset and save it to the path \"/content/imagedb_btsd\" :\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "local_zip = '/content/drive/MyDrive/cv_proj_4_dataset/imagedb_btsd.zip'\n",
        "\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/content/imagedb_btsd')\n",
        "zip_ref.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "23UpZ_4lTZGN"
      },
      "outputs": [],
      "source": [
        "# Select the folder \"imagedb\" as the training folder\n",
        "# and the folder \"imagedb_test\" as the testing folder :\n",
        "\n",
        "base_dir = '/content/imagedb_btsd'\n",
        "\n",
        "train_dir = os.path.join(base_dir, 'imagedb')\n",
        "test_dir = os.path.join(base_dir, 'imagedb_test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VAg9eIeSShP9",
        "outputId": "94f9fc50-8fed-4424-c31e-3c15558d0f51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 254, 254, 32)      896       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 127, 127, 32)     0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 125, 125, 64)      18496     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 62, 62, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 60, 60, 128)       73856     \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 30, 30, 128)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 28, 28, 256)       295168    \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 14, 14, 256)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 50176)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1024)              51381248  \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 34)                34850     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 51,804,514\n",
            "Trainable params: 51,804,514\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "from keras import optimizers\n",
        "\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (3, 3), padding=\"valid\", activation='relu', input_shape=(256, 256, 3)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(layers.Conv2D(64, (3, 3), padding=\"valid\", activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "# model.add(layers.Conv2D(64, (3, 3), padding=\"valid\", activation='relu'))\n",
        "model.add(layers.Conv2D(128, (3, 3), padding=\"valid\", activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "# model.add(layers.Conv2D(128, (3, 3), padding=\"valid\", activation='relu'))\n",
        "model.add(layers.Conv2D(256, (3, 3), padding=\"valid\", activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(1024, activation='relu'))\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(34, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer=optimizers.adam_v2.Adam(learning_rate=1e-4),\n",
        "              loss = 'categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWGtpHkotMbZ",
        "outputId": "10ce56b9-eeb6-4a51-dca0-e13bb1b99543"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<keras.layers.convolutional.Conv2D object at 0x7f5ea5fbcb50> True\n",
            "<keras.layers.pooling.MaxPooling2D object at 0x7f5e31127f50> True\n",
            "<keras.layers.convolutional.Conv2D object at 0x7f5e201667d0> True\n",
            "<keras.layers.pooling.MaxPooling2D object at 0x7f5e200dfe90> True\n",
            "<keras.layers.convolutional.Conv2D object at 0x7f5e200e3ad0> True\n",
            "<keras.layers.pooling.MaxPooling2D object at 0x7f5e200e3d10> True\n",
            "<keras.layers.convolutional.Conv2D object at 0x7f5e200df190> True\n",
            "<keras.layers.pooling.MaxPooling2D object at 0x7f5e20119cd0> True\n",
            "<keras.layers.core.flatten.Flatten object at 0x7f5e2007b7d0> True\n",
            "<keras.layers.core.dense.Dense object at 0x7f5e200ed390> True\n",
            "<keras.layers.core.dropout.Dropout object at 0x7f5e20166c50> True\n",
            "<keras.layers.core.dense.Dense object at 0x7f5e200e30d0> True\n"
          ]
        }
      ],
      "source": [
        "# check this :\n",
        "\n",
        "# Check the trainable status of the individual layers\n",
        "for layer in model.layers:\n",
        "    print(layer, layer.trainable)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWDZ2ZpnShK9",
        "outputId": "70db378d-023a-4b01-aaf1-20cbef56ebde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2457 images belonging to 34 classes.\n",
            "Found 599 images belonging to 34 classes.\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(rotation_range = 90,\n",
        "                                  horizontal_flip=True,\n",
        "                                  vertical_flip = True,\n",
        "                                   brightness_range=[0.2,1.0],\n",
        "                                   zoom_range=[0.5,1.0],                                   \n",
        "                                   validation_split=0.2)\n",
        "                                  #  preprocessing_function = preprocess_func)\n",
        "                                  \n",
        "#train_datagen  = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
        "\n",
        "# --------------------\n",
        "# Flow training images in batches using train_datagen generator\n",
        "# --------------------\n",
        "train_generator = train_datagen.flow_from_directory(train_dir,\n",
        "                                                    batch_size=80,\n",
        "                                                    class_mode='categorical',\n",
        "                                                    # color_mode='grayscale',\n",
        "                                                    target_size=(256,256),\n",
        "                                                    shuffle=True,\n",
        "                                                    subset='training', seed=1)     \n",
        "# --------------------\n",
        "# Flow validation images in batches using test_datagen generator\n",
        "# --------------------\n",
        "validation_generator =  train_datagen.flow_from_directory(train_dir,\n",
        "                                                          batch_size=80,\n",
        "                                                          class_mode='categorical',\n",
        "                                                          # color_mode='grayscale',\n",
        "                                                          target_size=(256,256),\n",
        "                                                          subset='validation', seed=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ueQ8b0eukBoi"
      },
      "outputs": [],
      "source": [
        "# Apply callbacks :\n",
        "\n",
        "import datetime \n",
        "import tensorflow as tf\n",
        "  \n",
        "my_callbacks = []\n",
        "\n",
        "# logdir = os.path.join(\"/content/logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "# tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
        "# my_callbacks.append(tensorboard_callback)\n",
        "\n",
        "save_best_callback = tf.keras.callbacks.ModelCheckpoint(f'model_from_scratch_best.hdf5', save_best_only=True, verbose=1)\n",
        "my_callbacks.append(save_best_callback)\n",
        "\n",
        "early_stop_callback = tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True, verbose=1)\n",
        "my_callbacks.append(early_stop_callback)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5iuGSf2TzI_",
        "outputId": "551a6248-c02a-40a4-9d17-32dfbaba07a5"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "31/30 [==============================] - ETA: 0s - loss: 14.1347 - accuracy: 0.1770WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 15 batches). You may need to use the repeat() function when building your dataset.\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 2.23137, saving model to model_from_scratch_best.hdf5\n",
            "30/30 [==============================] - 60s 2s/step - loss: 14.1347 - accuracy: 0.1770 - val_loss: 2.2314 - val_accuracy: 0.4624\n",
            "Epoch 2/30\n",
            "31/30 [==============================] - ETA: 0s - loss: 2.0464 - accuracy: 0.4957WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "30/30 [==============================] - 36s 1s/step - loss: 2.0464 - accuracy: 0.4957\n",
            "Epoch 3/30\n",
            "31/30 [==============================] - ETA: 0s - loss: 1.4957 - accuracy: 0.6081WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "30/30 [==============================] - 36s 1s/step - loss: 1.4957 - accuracy: 0.6081\n",
            "Epoch 4/30\n",
            "31/30 [==============================] - ETA: 0s - loss: 1.2222 - accuracy: 0.6732WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "30/30 [==============================] - 36s 1s/step - loss: 1.2222 - accuracy: 0.6732\n",
            "Epoch 5/30\n",
            "31/30 [==============================] - ETA: 0s - loss: 1.0858 - accuracy: 0.7163WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "30/30 [==============================] - 36s 1s/step - loss: 1.0858 - accuracy: 0.7163\n",
            "Epoch 6/30\n",
            "31/30 [==============================] - ETA: 0s - loss: 0.9036 - accuracy: 0.7570WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "30/30 [==============================] - 36s 1s/step - loss: 0.9036 - accuracy: 0.7570\n",
            "Epoch 7/30\n",
            "31/30 [==============================] - ETA: 0s - loss: 0.8224 - accuracy: 0.7863WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "30/30 [==============================] - 36s 1s/step - loss: 0.8224 - accuracy: 0.7863\n",
            "Epoch 8/30\n",
            "31/30 [==============================] - ETA: 0s - loss: 0.7587 - accuracy: 0.7908WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "30/30 [==============================] - 36s 1s/step - loss: 0.7587 - accuracy: 0.7908\n",
            "Epoch 9/30\n",
            "31/30 [==============================] - ETA: 0s - loss: 0.6882 - accuracy: 0.8209WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "30/30 [==============================] - 37s 1s/step - loss: 0.6882 - accuracy: 0.8209\n",
            "Epoch 10/30\n",
            "31/30 [==============================] - ETA: 0s - loss: 0.5991 - accuracy: 0.8360WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "30/30 [==============================] - 37s 1s/step - loss: 0.5991 - accuracy: 0.8360\n",
            "Epoch 11/30\n",
            "31/30 [==============================] - ETA: 0s - loss: 0.6019 - accuracy: 0.8445WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "30/30 [==============================] - 45s 1s/step - loss: 0.6019 - accuracy: 0.8445\n",
            "Epoch 12/30\n",
            "31/30 [==============================] - ETA: 0s - loss: 0.5828 - accuracy: 0.8413WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "30/30 [==============================] - 37s 1s/step - loss: 0.5828 - accuracy: 0.8413\n",
            "Epoch 13/30\n",
            "31/30 [==============================] - ETA: 0s - loss: 0.4902 - accuracy: 0.8641WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "30/30 [==============================] - 37s 1s/step - loss: 0.4902 - accuracy: 0.8641\n",
            "Epoch 14/30\n",
            "31/30 [==============================] - ETA: 0s - loss: 0.5242 - accuracy: 0.8592WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "30/30 [==============================] - 37s 1s/step - loss: 0.5242 - accuracy: 0.8592\n",
            "Epoch 15/30\n",
            "31/30 [==============================] - ETA: 0s - loss: 0.4972 - accuracy: 0.8706WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "30/30 [==============================] - 37s 1s/step - loss: 0.4972 - accuracy: 0.8706\n",
            "Epoch 16/30\n",
            "31/30 [==============================] - ETA: 0s - loss: 0.4620 - accuracy: 0.8767WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "30/30 [==============================] - 36s 1s/step - loss: 0.4620 - accuracy: 0.8767\n",
            "Epoch 17/30\n",
            "31/30 [==============================] - ETA: 0s - loss: 0.4698 - accuracy: 0.8795WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "30/30 [==============================] - 36s 1s/step - loss: 0.4698 - accuracy: 0.8795\n",
            "Epoch 18/30\n",
            "31/30 [==============================] - ETA: 0s - loss: 0.4313 - accuracy: 0.8832WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "30/30 [==============================] - 36s 1s/step - loss: 0.4313 - accuracy: 0.8832\n",
            "Epoch 19/30\n",
            "31/30 [==============================] - ETA: 0s - loss: 0.4248 - accuracy: 0.8893WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "30/30 [==============================] - 36s 1s/step - loss: 0.4248 - accuracy: 0.8893\n",
            "Epoch 20/30\n",
            "31/30 [==============================] - ETA: 0s - loss: 0.3968 - accuracy: 0.8905WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "30/30 [==============================] - 36s 1s/step - loss: 0.3968 - accuracy: 0.8905\n",
            "Epoch 21/30\n",
            "31/30 [==============================] - ETA: 0s - loss: 0.3792 - accuracy: 0.8995WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "30/30 [==============================] - 36s 1s/step - loss: 0.3792 - accuracy: 0.8995\n",
            "Epoch 22/30\n",
            "31/30 [==============================] - ETA: 0s - loss: 0.3617 - accuracy: 0.8995WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "30/30 [==============================] - 36s 1s/step - loss: 0.3617 - accuracy: 0.8995\n",
            "Epoch 23/30\n",
            "31/30 [==============================] - ETA: 0s - loss: 0.3502 - accuracy: 0.9035WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "30/30 [==============================] - 36s 1s/step - loss: 0.3502 - accuracy: 0.9035\n",
            "Epoch 24/30\n",
            "31/30 [==============================] - ETA: 0s - loss: 0.3471 - accuracy: 0.9084WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "30/30 [==============================] - 37s 1s/step - loss: 0.3471 - accuracy: 0.9084\n",
            "Epoch 25/30\n",
            "31/30 [==============================] - ETA: 0s - loss: 0.3432 - accuracy: 0.9072WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "30/30 [==============================] - 36s 1s/step - loss: 0.3432 - accuracy: 0.9072\n",
            "Epoch 26/30\n",
            "31/30 [==============================] - ETA: 0s - loss: 0.2929 - accuracy: 0.9174WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "30/30 [==============================] - 36s 1s/step - loss: 0.2929 - accuracy: 0.9174\n",
            "Epoch 27/30\n",
            "31/30 [==============================] - ETA: 0s - loss: 0.3199 - accuracy: 0.9162WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "30/30 [==============================] - 36s 1s/step - loss: 0.3199 - accuracy: 0.9162\n",
            "Epoch 28/30\n",
            "31/30 [==============================] - ETA: 0s - loss: 0.3131 - accuracy: 0.9141WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "30/30 [==============================] - 36s 1s/step - loss: 0.3131 - accuracy: 0.9141\n",
            "Epoch 29/30\n",
            "31/30 [==============================] - ETA: 0s - loss: 0.2923 - accuracy: 0.9231WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "30/30 [==============================] - 36s 1s/step - loss: 0.2923 - accuracy: 0.9231\n",
            "Epoch 30/30\n",
            "31/30 [==============================] - ETA: 0s - loss: 0.2771 - accuracy: 0.9292WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "30/30 [==============================] - 37s 1s/step - loss: 0.2771 - accuracy: 0.9292\n"
          ]
        }
      ],
      "source": [
        "history = model.fit_generator(train_generator,\n",
        "                              validation_data=validation_generator,\n",
        "                              # steps_per_epoch=50,\n",
        "                              steps_per_epoch=train_generator.samples/train_generator.batch_size,\n",
        "                              epochs=30,\n",
        "                              validation_steps=15,\n",
        "                              verbose=1,\n",
        "                              callbacks=my_callbacks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "BZANTHekSznu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e906407-8b85-4957-bc0c-e85f3711d662"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2149 images belonging to 34 classes.\n"
          ]
        }
      ],
      "source": [
        "# test_datagen  = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen  = ImageDataGenerator()\n",
        "# --------------------\n",
        "# Flow testing images in batches using test_datagen generator\n",
        "# --------------------\n",
        "test_generator =  test_datagen.flow_from_directory(test_dir,\n",
        "                                                   batch_size=10,\n",
        "                                                   class_mode='categorical',\n",
        "                                                  #  color_mode='grayscale',\n",
        "                                                   target_size=(256,256))\n",
        "# # Testing the CNN on testing data : \n",
        "# loss, acc = model.evaluate(test_generator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Q3PTISh9XtGh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "outputId": "2285d43c-215d-48ac-c75f-6cff64c7beb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "215/215 [==============================] - 4s 16ms/step - loss: 1.4146 - accuracy: 0.7585\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['accuracy'], label='accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([0, 1])\n",
        "plt.legend(loc='lower right')\n",
        "\n",
        "# Testing the CNN on testing data : \n",
        "loss, acc = model.evaluate(test_generator)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Loading the best state model from the content folder : \n",
        "\n",
        "model_saved = models.load_model('/content/model_from_scratch_best.hdf5')\n",
        "model_saved.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmXJP6VsLlS9",
        "outputId": "3d737fdb-cc49-46b3-d34e-264f8b84e267"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 254, 254, 32)      896       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 127, 127, 32)     0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 125, 125, 64)      18496     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 62, 62, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 60, 60, 128)       73856     \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 30, 30, 128)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 28, 28, 256)       295168    \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 14, 14, 256)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 50176)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1024)              51381248  \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 34)                34850     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 51,804,514\n",
            "Trainable params: 51,804,514\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " loss_saved, acc_saved = model_saved.evaluate(test_generator)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gD96ihm6LwpB",
        "outputId": "30ab28ac-d149-4b67-a98e-3040d4d1a90c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "215/215 [==============================] - 3s 15ms/step - loss: 2.5251 - accuracy: 0.2936\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "CV_Project_4_NOT_pretrained.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOEvtfIP6q/5C2WoE4hXnWb"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}