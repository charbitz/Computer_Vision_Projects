{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CV_Project_4_NOT_pretrained.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMxG7HylVoE4hUkZe4/AKoT"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1efK3dBnSNsW",
        "outputId": "f2c306b0-03c3-47cc-cfc1-21809748ac01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Mount google drive :\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzip the dataset and save it to the path \"/content/imagedb_btsd\" :\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "local_zip = '/content/drive/MyDrive/cv_proj_4_dataset/imagedb_btsd.zip'\n",
        "\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/content/imagedb_btsd')\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "id": "UES-_QQCSXQM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the folder \"imagedb\" as the training folder\n",
        "# and the folder \"imagedb_test\" as the testing folder :\n",
        "\n",
        "base_dir = '/content/imagedb_btsd'\n",
        "\n",
        "train_dir = os.path.join(base_dir, 'imagedb')\n",
        "test_dir = os.path.join(base_dir, 'imagedb_test')"
      ],
      "metadata": {
        "id": "23UpZ_4lTZGN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "from keras import optimizers\n",
        "\n",
        "\n",
        "# model = tf.keras.models.Sequential([\n",
        "#   tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "#   tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
        "#   tf.keras.layers.Dense(3, activation=tf.nn.softmax)\n",
        "# ])\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(256, 256, 3)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "# model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "# model.add(layers.Dropout(0.5))\n",
        "\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "# model.add(layers.Dropout(0.5))\n",
        "# model.add(layers.Conv2D(256, (3, 3), activation='relu'))\n",
        "# model.add(layers.MaxPooling2D((2, 2)))\n",
        "# model.add(layers.Dropout(0.5))\n",
        "# model.add(layers.Conv2D(512, (3, 3), activation='relu'))\n",
        "# model.add(layers.MaxPooling2D((2, 2)))\n",
        "# model.add(layers.Dropout(0.5))\n",
        "\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(1024, activation='relu'))\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(34, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer=optimizers.adam_v2.Adam(learning_rate=1e-4),\n",
        "              loss = 'categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VAg9eIeSShP9",
        "outputId": "3664a8fd-443b-4437-e522-22a06bc22413"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 254, 254, 32)      896       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 127, 127, 32)     0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 125, 125, 64)      18496     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 62, 62, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 60, 60, 64)        36928     \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 30, 30, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 28, 28, 128)       73856     \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 14, 14, 128)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 25088)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1024)              25691136  \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 34)                34850     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 25,856,162\n",
            "Trainable params: 25,856,162\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check this :\n",
        "\n",
        "# Check the trainable status of the individual layers\n",
        "for layer in model.layers:\n",
        "    print(layer, layer.trainable)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWGtpHkotMbZ",
        "outputId": "d128dd3d-3779-49d4-a564-07fc24f05729"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<keras.layers.convolutional.Conv2D object at 0x7f779ba15bd0> True\n",
            "<keras.layers.pooling.MaxPooling2D object at 0x7f7726b47fd0> True\n",
            "<keras.layers.convolutional.Conv2D object at 0x7f77263058d0> True\n",
            "<keras.layers.pooling.MaxPooling2D object at 0x7f77201299d0> True\n",
            "<keras.layers.convolutional.Conv2D object at 0x7f7726305d50> True\n",
            "<keras.layers.pooling.MaxPooling2D object at 0x7f77200c8c10> True\n",
            "<keras.layers.convolutional.Conv2D object at 0x7f77200d66d0> True\n",
            "<keras.layers.pooling.MaxPooling2D object at 0x7f77200d6f50> True\n",
            "<keras.layers.core.flatten.Flatten object at 0x7f77200d6d50> True\n",
            "<keras.layers.core.dense.Dense object at 0x7f77200cc710> True\n",
            "<keras.layers.core.dropout.Dropout object at 0x7f77200e98d0> True\n",
            "<keras.layers.core.dense.Dense object at 0x7f77200e91d0> True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(rotation_range = 180,\n",
        "                                  # horizontal_flip=True,\n",
        "                                  # vertical_flip = True,\n",
        "                                   brightness_range=[0.2,1.0],\n",
        "                                   zoom_range=[0.5,1.0],                                   \n",
        "                                   validation_split=0.2)\n",
        "                                  #  preprocessing_function = preprocess_func)\n",
        "                                  \n",
        "#train_datagen  = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
        "\n",
        "# --------------------\n",
        "# Flow training images in batches using train_datagen generator\n",
        "# --------------------\n",
        "train_generator = train_datagen.flow_from_directory(train_dir,\n",
        "                                                    batch_size=40,\n",
        "                                                    class_mode='categorical',\n",
        "                                                    # color_mode='grayscale',\n",
        "                                                    target_size=(256,256),\n",
        "                                                    shuffle=True,\n",
        "                                                    subset='training', seed=1)     \n",
        "# --------------------\n",
        "# Flow validation images in batches using test_datagen generator\n",
        "# --------------------\n",
        "validation_generator =  train_datagen.flow_from_directory(train_dir,\n",
        "                                                          batch_size=40,\n",
        "                                                          class_mode='categorical',\n",
        "                                                          # color_mode='grayscale',\n",
        "                                                          target_size=(256,256),\n",
        "                                                          subset='validation', seed=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWDZ2ZpnShK9",
        "outputId": "8a1738ec-06bc-4202-df3a-46c9178a040a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2457 images belonging to 34 classes.\n",
            "Found 599 images belonging to 34 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply callbacks :\n",
        "\n",
        "import datetime \n",
        "import tensorflow as tf\n",
        "  \n",
        "my_callbacks = []\n",
        "\n",
        "# logdir = os.path.join(\"/content/logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "# tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
        "# my_callbacks.append(tensorboard_callback)\n",
        "\n",
        "save_best_callback = tf.keras.callbacks.ModelCheckpoint(f'model_from_scratch_best.hdf5', save_best_only=True, verbose=1)\n",
        "my_callbacks.append(save_best_callback)\n",
        "\n",
        "early_stop_callback = tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True, verbose=1)\n",
        "my_callbacks.append(early_stop_callback)"
      ],
      "metadata": {
        "id": "ueQ8b0eukBoi"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit_generator(train_generator,\n",
        "                              validation_data=validation_generator,\n",
        "                              # steps_per_epoch=50,\n",
        "                              steps_per_epoch=train_generator.samples/train_generator.batch_size,\n",
        "                              epochs=30,\n",
        "                              validation_steps=15,\n",
        "                              verbose=1,\n",
        "                              callbacks=my_callbacks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5iuGSf2TzI_",
        "outputId": "be7a30ac-3988-4ce9-bf6d-fc199088dfab"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "62/61 [==============================] - ETA: 0s - loss: 7.6876 - accuracy: 0.2263\n",
            "Epoch 00001: val_loss improved from inf to 2.09307, saving model to model_from_scratch_best.hdf5\n",
            "61/61 [==============================] - 66s 961ms/step - loss: 7.6876 - accuracy: 0.2263 - val_loss: 2.0931 - val_accuracy: 0.4307\n",
            "Epoch 2/30\n",
            "62/61 [==============================] - ETA: 0s - loss: 2.1710 - accuracy: 0.4420\n",
            "Epoch 00002: val_loss improved from 2.09307 to 1.53991, saving model to model_from_scratch_best.hdf5\n",
            "61/61 [==============================] - 55s 897ms/step - loss: 2.1710 - accuracy: 0.4420 - val_loss: 1.5399 - val_accuracy: 0.5843\n",
            "Epoch 3/30\n",
            "62/61 [==============================] - ETA: 0s - loss: 1.7803 - accuracy: 0.5254\n",
            "Epoch 00003: val_loss improved from 1.53991 to 1.24532, saving model to model_from_scratch_best.hdf5\n",
            "61/61 [==============================] - 53s 864ms/step - loss: 1.7803 - accuracy: 0.5254 - val_loss: 1.2453 - val_accuracy: 0.6661\n",
            "Epoch 4/30\n",
            "62/61 [==============================] - ETA: 0s - loss: 1.4967 - accuracy: 0.5971\n",
            "Epoch 00004: val_loss improved from 1.24532 to 1.05789, saving model to model_from_scratch_best.hdf5\n",
            "61/61 [==============================] - 53s 856ms/step - loss: 1.4967 - accuracy: 0.5971 - val_loss: 1.0579 - val_accuracy: 0.7195\n",
            "Epoch 5/30\n",
            "62/61 [==============================] - ETA: 0s - loss: 1.2942 - accuracy: 0.6492\n",
            "Epoch 00005: val_loss improved from 1.05789 to 0.96527, saving model to model_from_scratch_best.hdf5\n",
            "61/61 [==============================] - 54s 875ms/step - loss: 1.2942 - accuracy: 0.6492 - val_loss: 0.9653 - val_accuracy: 0.7362\n",
            "Epoch 6/30\n",
            "62/61 [==============================] - ETA: 0s - loss: 1.1437 - accuracy: 0.6866\n",
            "Epoch 00006: val_loss improved from 0.96527 to 0.86406, saving model to model_from_scratch_best.hdf5\n",
            "61/61 [==============================] - 52s 854ms/step - loss: 1.1437 - accuracy: 0.6866 - val_loss: 0.8641 - val_accuracy: 0.7629\n",
            "Epoch 7/30\n",
            "62/61 [==============================] - ETA: 0s - loss: 1.0022 - accuracy: 0.7302\n",
            "Epoch 00007: val_loss improved from 0.86406 to 0.84361, saving model to model_from_scratch_best.hdf5\n",
            "61/61 [==============================] - 53s 862ms/step - loss: 1.0022 - accuracy: 0.7302 - val_loss: 0.8436 - val_accuracy: 0.7846\n",
            "Epoch 8/30\n",
            "62/61 [==============================] - ETA: 0s - loss: 0.9118 - accuracy: 0.7566\n",
            "Epoch 00008: val_loss improved from 0.84361 to 0.67034, saving model to model_from_scratch_best.hdf5\n",
            "61/61 [==============================] - 53s 859ms/step - loss: 0.9118 - accuracy: 0.7566 - val_loss: 0.6703 - val_accuracy: 0.8364\n",
            "Epoch 9/30\n",
            "62/61 [==============================] - ETA: 0s - loss: 0.8207 - accuracy: 0.7859\n",
            "Epoch 00009: val_loss improved from 0.67034 to 0.65415, saving model to model_from_scratch_best.hdf5\n",
            "61/61 [==============================] - 53s 861ms/step - loss: 0.8207 - accuracy: 0.7859 - val_loss: 0.6542 - val_accuracy: 0.8197\n",
            "Epoch 10/30\n",
            "62/61 [==============================] - ETA: 0s - loss: 0.7513 - accuracy: 0.7957\n",
            "Epoch 00010: val_loss improved from 0.65415 to 0.60541, saving model to model_from_scratch_best.hdf5\n",
            "61/61 [==============================] - 54s 876ms/step - loss: 0.7513 - accuracy: 0.7957 - val_loss: 0.6054 - val_accuracy: 0.8414\n",
            "Epoch 11/30\n",
            "62/61 [==============================] - ETA: 0s - loss: 0.6503 - accuracy: 0.8254\n",
            "Epoch 00011: val_loss improved from 0.60541 to 0.55951, saving model to model_from_scratch_best.hdf5\n",
            "61/61 [==============================] - 53s 863ms/step - loss: 0.6503 - accuracy: 0.8254 - val_loss: 0.5595 - val_accuracy: 0.8614\n",
            "Epoch 12/30\n",
            "62/61 [==============================] - ETA: 0s - loss: 0.6656 - accuracy: 0.8205\n",
            "Epoch 00012: val_loss improved from 0.55951 to 0.54228, saving model to model_from_scratch_best.hdf5\n",
            "61/61 [==============================] - 53s 868ms/step - loss: 0.6656 - accuracy: 0.8205 - val_loss: 0.5423 - val_accuracy: 0.8514\n",
            "Epoch 13/30\n",
            "62/61 [==============================] - ETA: 0s - loss: 0.5974 - accuracy: 0.8327\n",
            "Epoch 00013: val_loss improved from 0.54228 to 0.51120, saving model to model_from_scratch_best.hdf5\n",
            "61/61 [==============================] - 54s 883ms/step - loss: 0.5974 - accuracy: 0.8327 - val_loss: 0.5112 - val_accuracy: 0.8548\n",
            "Epoch 14/30\n",
            "62/61 [==============================] - ETA: 0s - loss: 0.6235 - accuracy: 0.8356\n",
            "Epoch 00014: val_loss did not improve from 0.51120\n",
            "61/61 [==============================] - 53s 870ms/step - loss: 0.6235 - accuracy: 0.8356 - val_loss: 0.5550 - val_accuracy: 0.8431\n",
            "Epoch 15/30\n",
            "62/61 [==============================] - ETA: 0s - loss: 0.5790 - accuracy: 0.8384\n",
            "Epoch 00015: val_loss did not improve from 0.51120\n",
            "61/61 [==============================] - 53s 858ms/step - loss: 0.5790 - accuracy: 0.8384 - val_loss: 0.5597 - val_accuracy: 0.8531\n",
            "Epoch 16/30\n",
            "62/61 [==============================] - ETA: 0s - loss: 0.5223 - accuracy: 0.8624\n",
            "Epoch 00016: val_loss improved from 0.51120 to 0.44007, saving model to model_from_scratch_best.hdf5\n",
            "61/61 [==============================] - 53s 866ms/step - loss: 0.5223 - accuracy: 0.8624 - val_loss: 0.4401 - val_accuracy: 0.8798\n",
            "Epoch 17/30\n",
            "62/61 [==============================] - ETA: 0s - loss: 0.5118 - accuracy: 0.8531\n",
            "Epoch 00017: val_loss did not improve from 0.44007\n",
            "61/61 [==============================] - 53s 867ms/step - loss: 0.5118 - accuracy: 0.8531 - val_loss: 0.5362 - val_accuracy: 0.8497\n",
            "Epoch 18/30\n",
            "62/61 [==============================] - ETA: 0s - loss: 0.4703 - accuracy: 0.8718\n",
            "Epoch 00018: val_loss did not improve from 0.44007\n",
            "61/61 [==============================] - 53s 863ms/step - loss: 0.4703 - accuracy: 0.8718 - val_loss: 0.4905 - val_accuracy: 0.8765\n",
            "Epoch 19/30\n",
            "62/61 [==============================] - ETA: 0s - loss: 0.4490 - accuracy: 0.8820\n",
            "Epoch 00019: val_loss did not improve from 0.44007\n",
            "61/61 [==============================] - 52s 850ms/step - loss: 0.4490 - accuracy: 0.8820 - val_loss: 0.4435 - val_accuracy: 0.8831\n",
            "Epoch 20/30\n",
            "62/61 [==============================] - ETA: 0s - loss: 0.4196 - accuracy: 0.8852\n",
            "Epoch 00020: val_loss did not improve from 0.44007\n",
            "61/61 [==============================] - 52s 853ms/step - loss: 0.4196 - accuracy: 0.8852 - val_loss: 0.5147 - val_accuracy: 0.8514\n",
            "Epoch 21/30\n",
            "62/61 [==============================] - ETA: 0s - loss: 0.4298 - accuracy: 0.8856\n",
            "Epoch 00021: val_loss did not improve from 0.44007\n",
            "61/61 [==============================] - 53s 858ms/step - loss: 0.4298 - accuracy: 0.8856 - val_loss: 0.4477 - val_accuracy: 0.8915\n",
            "Epoch 22/30\n",
            "62/61 [==============================] - ETA: 0s - loss: 0.4236 - accuracy: 0.8897\n",
            "Epoch 00022: val_loss did not improve from 0.44007\n",
            "61/61 [==============================] - 53s 869ms/step - loss: 0.4236 - accuracy: 0.8897 - val_loss: 0.5681 - val_accuracy: 0.8598\n",
            "Epoch 23/30\n",
            "62/61 [==============================] - ETA: 0s - loss: 0.4129 - accuracy: 0.8913\n",
            "Epoch 00023: val_loss improved from 0.44007 to 0.43998, saving model to model_from_scratch_best.hdf5\n",
            "61/61 [==============================] - 53s 869ms/step - loss: 0.4129 - accuracy: 0.8913 - val_loss: 0.4400 - val_accuracy: 0.8815\n",
            "Epoch 24/30\n",
            "62/61 [==============================] - ETA: 0s - loss: 0.3486 - accuracy: 0.8987\n",
            "Epoch 00024: val_loss did not improve from 0.43998\n",
            "61/61 [==============================] - 53s 857ms/step - loss: 0.3486 - accuracy: 0.8987 - val_loss: 0.4742 - val_accuracy: 0.8915\n",
            "Epoch 25/30\n",
            "62/61 [==============================] - ETA: 0s - loss: 0.3774 - accuracy: 0.9027\n",
            "Epoch 00025: val_loss improved from 0.43998 to 0.40649, saving model to model_from_scratch_best.hdf5\n",
            "61/61 [==============================] - 65s 1s/step - loss: 0.3774 - accuracy: 0.9027 - val_loss: 0.4065 - val_accuracy: 0.8731\n",
            "Epoch 26/30\n",
            "62/61 [==============================] - ETA: 0s - loss: 0.3761 - accuracy: 0.8987\n",
            "Epoch 00026: val_loss improved from 0.40649 to 0.39174, saving model to model_from_scratch_best.hdf5\n",
            "61/61 [==============================] - 54s 872ms/step - loss: 0.3761 - accuracy: 0.8987 - val_loss: 0.3917 - val_accuracy: 0.8865\n",
            "Epoch 27/30\n",
            "62/61 [==============================] - ETA: 0s - loss: 0.3246 - accuracy: 0.9052\n",
            "Epoch 00027: val_loss improved from 0.39174 to 0.36744, saving model to model_from_scratch_best.hdf5\n",
            "61/61 [==============================] - 55s 889ms/step - loss: 0.3246 - accuracy: 0.9052 - val_loss: 0.3674 - val_accuracy: 0.8982\n",
            "Epoch 28/30\n",
            "62/61 [==============================] - ETA: 0s - loss: 0.3150 - accuracy: 0.9109\n",
            "Epoch 00028: val_loss did not improve from 0.36744\n",
            "61/61 [==============================] - 53s 864ms/step - loss: 0.3150 - accuracy: 0.9109 - val_loss: 0.3875 - val_accuracy: 0.8915\n",
            "Epoch 29/30\n",
            "62/61 [==============================] - ETA: 0s - loss: 0.3217 - accuracy: 0.9158\n",
            "Epoch 00029: val_loss did not improve from 0.36744\n",
            "61/61 [==============================] - 54s 872ms/step - loss: 0.3217 - accuracy: 0.9158 - val_loss: 0.4790 - val_accuracy: 0.8831\n",
            "Epoch 30/30\n",
            "62/61 [==============================] - ETA: 0s - loss: 0.3117 - accuracy: 0.9133\n",
            "Epoch 00030: val_loss did not improve from 0.36744\n",
            "61/61 [==============================] - 54s 875ms/step - loss: 0.3117 - accuracy: 0.9133 - val_loss: 0.4324 - val_accuracy: 0.8765\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test_datagen  = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen  = ImageDataGenerator()\n",
        "# --------------------\n",
        "# Flow testing images in batches using test_datagen generator\n",
        "# --------------------\n",
        "test_generator =  test_datagen.flow_from_directory(test_dir,\n",
        "                                                   batch_size=10,\n",
        "                                                   class_mode='categorical',\n",
        "                                                  #  color_mode='grayscale',\n",
        "                                                   target_size=(256,256))\n",
        "# # Testing the CNN on testing data : \n",
        "# loss, acc = model.evaluate(test_generator)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZANTHekSznu",
        "outputId": "9fb11147-e7ce-42c3-97b3-555f772e6e66"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2149 images belonging to 34 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['accuracy'], label='accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([0, 1])\n",
        "plt.legend(loc='lower right')\n",
        "\n",
        "# Testing the CNN on testing data : \n",
        "loss, acc = model.evaluate(test_generator)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "Q3PTISh9XtGh",
        "outputId": "81285a73-5aa1-4159-e633-26e41cfd5e2f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "215/215 [==============================] - 5s 21ms/step - loss: 1.5653 - accuracy: 0.7180\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TRY LOADING THE VGG16 MODEL TO TEST IT WITHOUT rescale=1./255 :"
      ],
      "metadata": {
        "id": "qLiZCwTwVkMo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the model from the google drive folder : \n",
        "\n",
        "model_loaded = models.load_model('/content/drive/MyDrive/CV_Project_4_saved_models/pretrained_VGG16_batch_40_epochs_60_PROTECT')\n",
        "model_loaded.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ybgTKSCvVjv1",
        "outputId": "34e0484c-6934-4e15-c022-562faee0f4ba"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " vgg16 (Functional)          (None, 8, 8, 512)         14714688  \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 32768)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1024)              33555456  \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 34)                34850     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 48,304,994\n",
            "Trainable params: 40,669,730\n",
            "Non-trainable params: 7,635,264\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test_datagen  = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen_loaded  = ImageDataGenerator()\n",
        "# --------------------\n",
        "# Flow testing images in batches using test_datagen generator\n",
        "# --------------------\n",
        "test_generator_loaded =  test_datagen_loaded.flow_from_directory(test_dir,\n",
        "                                                   batch_size=10,\n",
        "                                                   class_mode='categorical',\n",
        "                                                  #  color_mode='grayscale',\n",
        "                                                   target_size=(256,256))\n",
        "# Testing the CNN on testing data : \n",
        "loss_loaded, acc_loaded = model_loaded.evaluate(test_generator_loaded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Mb1EyMvWLoN",
        "outputId": "3209fa9d-105c-4df0-b4a2-b848bbd3cb4d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2149 images belonging to 34 classes.\n",
            "215/215 [==============================] - 39s 159ms/step - loss: 0.5989 - acc: 0.8948\n"
          ]
        }
      ]
    }
  ]
}